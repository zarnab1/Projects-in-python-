{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.3.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\H T\n",
      "['obj1__0.png', 'obj1__1.png', 'obj1__10.png', 'obj1__11.png', 'obj1__12.png', 'obj1__13.png', 'obj1__14.png', 'obj1__15.png', 'obj1__16.png', 'obj1__17.png', 'obj1__18.png', 'obj1__19.png', 'obj1__2.png', 'obj1__20.png', 'obj1__21.png', 'obj1__22.png', 'obj1__23.png', 'obj1__24.png', 'obj1__25.png', 'obj1__26.png', 'obj1__27.png', 'obj1__28.png', 'obj1__29.png', 'obj1__3.png', 'obj1__30.png', 'obj1__31.png', 'obj1__32.png', 'obj1__33.png', 'obj1__34.png', 'obj1__35.png', 'obj1__36.png', 'obj1__37.png', 'obj1__38.png', 'obj1__39.png', 'obj1__4.png', 'obj1__40.png', 'obj1__41.png', 'obj1__42.png', 'obj1__43.png', 'obj1__44.png', 'obj1__45.png', 'obj1__46.png', 'obj1__47.png', 'obj1__48.png', 'obj1__49.png', 'obj1__5.png', 'obj1__50.png', 'obj1__51.png', 'obj1__52.png', 'obj1__53.png', 'obj1__54.png', 'obj1__55.png', 'obj1__56.png', 'obj1__57.png', 'obj1__58.png', 'obj1__59.png', 'obj1__6.png', 'obj1__60.png', 'obj1__61.png', 'obj1__62.png', 'obj1__63.png', 'obj1__64.png', 'obj1__65.png', 'obj1__66.png', 'obj1__67.png', 'obj1__68.png', 'obj1__69.png', 'obj1__7.png', 'obj1__70.png', 'obj1__71.png', 'obj1__8.png', 'obj1__9.png', 'obj2__0.png', 'obj2__1.png', 'obj2__10.png', 'obj2__11.png', 'obj2__12.png', 'obj2__13.png', 'obj2__14.png', 'obj2__15.png', 'obj2__16.png', 'obj2__17.png', 'obj2__18.png', 'obj2__19.png', 'obj2__2.png', 'obj2__20.png', 'obj2__21.png', 'obj2__22.png', 'obj2__23.png', 'obj2__24.png', 'obj2__25.png', 'obj2__26.png', 'obj2__27.png', 'obj2__28.png', 'obj2__29.png', 'obj2__3.png', 'obj2__30.png', 'obj2__31.png', 'obj2__32.png', 'obj2__33.png', 'obj2__34.png', 'obj2__35.png', 'obj2__36.png', 'obj2__37.png', 'obj2__38.png', 'obj2__39.png', 'obj2__4.png', 'obj2__40.png', 'obj2__41.png', 'obj2__42.png', 'obj2__43.png', 'obj2__44.png', 'obj2__45.png', 'obj2__46.png', 'obj2__47.png', 'obj2__48.png', 'obj2__49.png', 'obj2__5.png', 'obj2__50.png', 'obj2__51.png', 'obj2__52.png', 'obj2__53.png', 'obj2__54.png', 'obj2__55.png', 'obj2__56.png', 'obj2__57.png', 'obj2__58.png', 'obj2__59.png', 'obj2__6.png', 'obj2__60.png', 'obj2__61.png', 'obj2__62.png', 'obj2__63.png', 'obj2__64.png', 'obj2__65.png', 'obj2__66.png', 'obj2__67.png', 'obj2__68.png', 'obj2__69.png', 'obj2__7.png', 'obj2__70.png', 'obj2__71.png', 'obj2__8.png', 'obj2__9.png', 'obj3__0.png', 'obj3__1.png', 'obj3__10.png', 'obj3__11.png', 'obj3__12.png', 'obj3__13.png', 'obj3__14.png', 'obj3__15.png', 'obj3__16.png', 'obj3__17.png', 'obj3__18.png', 'obj3__19.png', 'obj3__2.png', 'obj3__20.png', 'obj3__21.png', 'obj3__22.png', 'obj3__23.png', 'obj3__24.png', 'obj3__25.png', 'obj3__26.png', 'obj3__27.png', 'obj3__28.png', 'obj3__29.png', 'obj3__3.png', 'obj3__30.png', 'obj3__31.png', 'obj3__32.png', 'obj3__33.png', 'obj3__34.png', 'obj3__35.png', 'obj3__36.png', 'obj3__37.png', 'obj3__38.png', 'obj3__39.png', 'obj3__4.png', 'obj3__40.png', 'obj3__41.png', 'obj3__42.png', 'obj3__43.png', 'obj3__44.png', 'obj3__45.png', 'obj3__46.png', 'obj3__47.png', 'obj3__48.png', 'obj3__49.png', 'obj3__5.png', 'obj3__50.png', 'obj3__51.png', 'obj3__52.png', 'obj3__53.png', 'obj3__54.png', 'obj3__55.png', 'obj3__56.png', 'obj3__57.png', 'obj3__58.png', 'obj3__59.png', 'obj3__6.png', 'obj3__60.png', 'obj3__61.png', 'obj3__62.png', 'obj3__63.png', 'obj3__64.png', 'obj3__65.png', 'obj3__66.png', 'obj3__67.png', 'obj3__68.png', 'obj3__69.png', 'obj3__7.png', 'obj3__70.png', 'obj3__71.png', 'obj3__8.png', 'obj3__9.png', 'obj4__0.png', 'obj4__1.png', 'obj4__10.png', 'obj4__11.png', 'obj4__12.png', 'obj4__13.png', 'obj4__14.png', 'obj4__15.png', 'obj4__16.png', 'obj4__17.png', 'obj4__18.png', 'obj4__19.png', 'obj4__2.png', 'obj4__20.png', 'obj4__21.png', 'obj4__22.png', 'obj4__23.png', 'obj4__24.png', 'obj4__25.png', 'obj4__26.png', 'obj4__27.png', 'obj4__28.png', 'obj4__29.png', 'obj4__3.png', 'obj4__30.png', 'obj4__31.png', 'obj4__32.png', 'obj4__33.png', 'obj4__34.png', 'obj4__35.png', 'obj4__36.png', 'obj4__37.png', 'obj4__38.png', 'obj4__39.png', 'obj4__4.png', 'obj4__40.png', 'obj4__41.png', 'obj4__42.png', 'obj4__43.png', 'obj4__44.png', 'obj4__45.png', 'obj4__46.png', 'obj4__47.png', 'obj4__48.png', 'obj4__49.png', 'obj4__5.png', 'obj4__50.png', 'obj4__51.png', 'obj4__52.png', 'obj4__53.png', 'obj4__54.png', 'obj4__55.png', 'obj4__56.png', 'obj4__57.png', 'obj4__58.png', 'obj4__59.png', 'obj4__6.png', 'obj4__60.png', 'obj4__61.png', 'obj4__62.png', 'obj4__63.png', 'obj4__64.png', 'obj4__65.png', 'obj4__66.png', 'obj4__67.png', 'obj4__68.png', 'obj4__69.png', 'obj4__7.png', 'obj4__70.png', 'obj4__71.png', 'obj4__8.png', 'obj4__9.png', 'obj5__0.png', 'obj5__1.png', 'obj5__10.png', 'obj5__11.png', 'obj5__12.png', 'obj5__13.png', 'obj5__14.png', 'obj5__15.png', 'obj5__16.png', 'obj5__17.png', 'obj5__18.png', 'obj5__19.png', 'obj5__2.png', 'obj5__20.png', 'obj5__21.png', 'obj5__22.png', 'obj5__23.png', 'obj5__24.png', 'obj5__25.png', 'obj5__26.png', 'obj5__27.png', 'obj5__28.png', 'obj5__29.png', 'obj5__3.png', 'obj5__30.png', 'obj5__31.png', 'obj5__32.png', 'obj5__33.png', 'obj5__34.png', 'obj5__35.png', 'obj5__36.png', 'obj5__37.png', 'obj5__38.png', 'obj5__39.png', 'obj5__4.png', 'obj5__40.png', 'obj5__41.png', 'obj5__42.png', 'obj5__43.png', 'obj5__44.png', 'obj5__45.png', 'obj5__46.png', 'obj5__47.png', 'obj5__48.png', 'obj5__49.png', 'obj5__5.png', 'obj5__50.png', 'obj5__51.png', 'obj5__52.png', 'obj5__53.png', 'obj5__54.png', 'obj5__55.png', 'obj5__56.png', 'obj5__57.png', 'obj5__58.png', 'obj5__59.png', 'obj5__6.png', 'obj5__60.png', 'obj5__61.png', 'obj5__62.png', 'obj5__63.png', 'obj5__64.png', 'obj5__65.png', 'obj5__66.png', 'obj5__67.png', 'obj5__68.png', 'obj5__69.png', 'obj5__7.png', 'obj5__70.png', 'obj5__71.png', 'obj5__8.png', 'obj5__9.png', 'prepared']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "#print(os.listdir())\n",
    "path='F:/8th Semester/Data Science/Machine Lerning/coil-20-unproc'\n",
    "print(os.listdir(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "original_dataset_dir = 'F:/8th Semester/Data Science/Machine Lerning/coil-20-unproc'\n",
    "base_dir = 'F:/8th Semester/Data Science/Machine Lerning/coil-20-unproc/prepared'\n",
    "if (not(os.path.exists(base_dir))):\n",
    "    os.mkdir(base_dir)\n",
    "\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "os.mkdir(train_dir)\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "os.mkdir(validation_dir)\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "os.mkdir(test_dir)\n",
    "\n",
    "# Directory with our training obj1 pictures\n",
    "train_obj1_dir = os.path.join(train_dir, 'obj1')\n",
    "os.mkdir(train_obj1_dir)\n",
    "\n",
    "# Directory with our training obj2 pictures\n",
    "train_obj2_dir = os.path.join(train_dir, 'obj2')\n",
    "os.mkdir(train_obj2_dir)\n",
    "\n",
    "# Directory with our training obj3 pictures\n",
    "train_obj3_dir = os.path.join(train_dir, 'obj3')\n",
    "os.mkdir(train_obj3_dir)\n",
    "\n",
    "# Directory with our training obj4 pictures\n",
    "train_obj4_dir = os.path.join(train_dir, 'obj4')\n",
    "os.mkdir(train_obj4_dir)\n",
    "\n",
    "# Directory with our training obj5 pictures\n",
    "train_obj5_dir = os.path.join(train_dir, 'obj5')\n",
    "os.mkdir(train_obj5_dir)\n",
    "\n",
    "# Directory with our validation cat pictures\n",
    "validation_obj1_dir = os.path.join(validation_dir, 'obj1')\n",
    "os.mkdir(validation_obj1_dir)\n",
    "\n",
    "# Directory with our validation dog pictures\n",
    "validation_obj2_dir = os.path.join(validation_dir, 'obj2')\n",
    "os.mkdir(validation_obj2_dir)\n",
    "\n",
    "validation_obj3_dir = os.path.join(validation_dir, 'obj3')\n",
    "os.mkdir(validation_obj3_dir)\n",
    "\n",
    "validation_obj4_dir = os.path.join(validation_dir, 'obj4')\n",
    "os.mkdir(validation_obj4_dir)\n",
    "\n",
    "validation_obj5_dir = os.path.join(validation_dir, 'obj5')\n",
    "os.mkdir(validation_obj5_dir)\n",
    "\n",
    "# Directory with our validation cat pictures\n",
    "test_obj1_dir = os.path.join(test_dir, 'obj1')\n",
    "os.mkdir(test_obj1_dir)\n",
    "\n",
    "# Directory with our validation dog pictures\n",
    "test_obj2_dir = os.path.join(test_dir, 'obj2')\n",
    "os.mkdir(test_obj2_dir)\n",
    "\n",
    "test_obj3_dir = os.path.join(test_dir, 'obj3')\n",
    "os.mkdir(test_obj3_dir)\n",
    "\n",
    "test_obj4_dir = os.path.join(test_dir, 'obj4')\n",
    "os.mkdir(test_obj4_dir)\n",
    "\n",
    "test_obj5_dir = os.path.join(test_dir, 'obj5')\n",
    "os.mkdir(test_obj5_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'F:/8th Semester/Data Science/Machine Lerning/coil-20-unproc/prepared\\\\train\\\\obj1'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fnames = ['obj1__{}.png'.format(i) for i in range(1000)]\n",
    "train_obj1_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obj1 (Train/Test) Completed.\n",
      "Obj2 (Train/Test) Completed.\n",
      "Obj3 (Train/Test) Completed.\n",
      "Obj4 (Train/Test) Completed.\n",
      "Obj5 (Train/Test) Completed.\n"
     ]
    }
   ],
   "source": [
    "#------------------obj1 train, test and validate \n",
    "# Copy 180 obj1 images to train_obj1_dir\n",
    "fnames = ['obj1__{}.png'.format(i) for i in range(35)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(train_obj2_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "\n",
    "# Copy 90 obj1  images to validation_obj1_dir\n",
    "fnames = ['obj1__{}.png'.format(i) for i in range(35,53)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(validation_obj1_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "\n",
    "# Copy 90 obj1 images to test_obj1_dir\n",
    "fnames = ['obj1__{}.png'.format(i) for i in range(53,71)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(test_obj1_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "\n",
    "print(\"Obj1 (Train/Test) Completed.\")\n",
    "\n",
    "\n",
    "#------------------obj2 train, test and validate \n",
    "# Copy 180 obj2 images to train_obj2_dir\n",
    "fnames = ['obj2__{}.png'.format(i) for i in range(35)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(train_obj2_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "    \n",
    "# Copy 90 obj2 images to validation_obj2_dir\n",
    "fnames = ['obj2__{}.png'.format(i) for i in range(35,53)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(validation_obj2_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "    \n",
    "# Copy 90 obj2 images to test_ob2_dir\n",
    "fnames = ['obj2__{}.png'.format(i) for i in range(53,71)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(test_obj2_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "    \n",
    "print(\"Obj2 (Train/Test) Completed.\")\n",
    "\n",
    "\n",
    "#------------------obj3 train, test and validate \n",
    "# Copy 180 obj3 images to train_obj3_dir\n",
    "fnames = ['obj3__{}.png'.format(i) for i in range(35)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(train_obj3_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "    \n",
    "# Copy 90 obj3 images to validation_obj3_dir\n",
    "fnames = ['obj3__{}.png'.format(i) for i in range(35,53)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(validation_obj3_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "    \n",
    "# Copy 90 obj3 images to test_obj3_dir\n",
    "fnames = ['obj3__{}.png'.format(i) for i in range(53,71)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(test_obj3_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "    \n",
    "print(\"Obj3 (Train/Test) Completed.\")\n",
    "\n",
    "\n",
    "#------------------obj4 train, test and validate \n",
    "# Copy 180 obj4 images to train_obj4_dir\n",
    "fnames = ['obj4__{}.png'.format(i) for i in range(35)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(train_obj4_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "    \n",
    "# Copy 90 obj4 images to validation_obj4_dir\n",
    "fnames = ['obj4__{}.png'.format(i) for i in range(35,53)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(validation_obj4_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "    \n",
    "# Copy 90 obj4 images to test_obj4_dir\n",
    "fnames = ['obj4__{}.png'.format(i) for i in range(53,71)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(test_obj4_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "    \n",
    "print(\"Obj4 (Train/Test) Completed.\")\n",
    "\n",
    "\n",
    "#------------------obj5 train, test and validate \n",
    "# Copy 180 obj5 images to train_obj5_dir\n",
    "fnames = ['obj5__{}.png'.format(i) for i in range(35)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(train_obj5_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "    \n",
    "# Copy 90 obj5 images to validation_obj5_dir\n",
    "fnames = ['obj5__{}.png'.format(i) for i in range(35,53)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(validation_obj5_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "    \n",
    "# Copy 90 obj5 images to test_obj5_dir\n",
    "fnames = ['obj5__{}.png'.format(i) for i in range(53,71)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(test_obj5_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "    \n",
    "print(\"Obj5 (Train/Test) Completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check, let's count how many pictures we have in each training split (train/validation/test):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total training obj1 images: 0\n"
     ]
    }
   ],
   "source": [
    "print('total training obj1 images:', len(os.listdir(train_obj1_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total training obj2 images: 70\n"
     ]
    }
   ],
   "source": [
    "print('total training obj2 images:', len(os.listdir(train_obj2_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total training obj3 images: 35\n"
     ]
    }
   ],
   "source": [
    "print('total training obj3 images:', len(os.listdir(train_obj3_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total training obj4 images: 35\n"
     ]
    }
   ],
   "source": [
    "print('total training obj4 images:', len(os.listdir(train_obj4_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total training obj5 images: 35\n"
     ]
    }
   ],
   "source": [
    "print('total training obj5 images:', len(os.listdir(train_obj5_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total validation obj1 images: 18\n"
     ]
    }
   ],
   "source": [
    "print('total validation obj1 images:', len(os.listdir(validation_obj1_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total validation obj2 images: 18\n"
     ]
    }
   ],
   "source": [
    "print('total validation obj2 images:', len(os.listdir(validation_obj2_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total validation obj3 images: 18\n"
     ]
    }
   ],
   "source": [
    "print('total validation obj3 images:', len(os.listdir(validation_obj3_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total validation obj4 images: 18\n"
     ]
    }
   ],
   "source": [
    "print('total validation obj4 images:', len(os.listdir(validation_obj4_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total validation obj5 images: 18\n"
     ]
    }
   ],
   "source": [
    "print('total validation obj5 images:', len(os.listdir(validation_obj5_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total test obj1 images: 18\n"
     ]
    }
   ],
   "source": [
    "print('total test obj1 images:', len(os.listdir(test_obj1_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total test obj2 images: 18\n"
     ]
    }
   ],
   "source": [
    "print('total test obj2 images:', len(os.listdir(test_obj2_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total test obj3 images: 18\n"
     ]
    }
   ],
   "source": [
    "print('total test obj3 images:', len(os.listdir(test_obj3_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total test obj4 images: 18\n"
     ]
    }
   ],
   "source": [
    "print('total test obj4 images:', len(os.listdir(test_obj4_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total test obj5 images: 18\n"
     ]
    }
   ],
   "source": [
    "print('total test obj5 images:', len(os.listdir(test_obj5_dir)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "So we have indeed 2000 training images, and then 1000 validation images and 1000 test images. In each split, there is the same number of \n",
    "samples from each class: this is a balanced binary classification problem, which means that classification accuracy will be an appropriate \n",
    "measure of success."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building our network\n",
    "\n",
    "We've already built a small convnet for MNIST in the previous example, so you should be familiar with them. We will reuse the same \n",
    "general structure: our convnet will be a stack of alternated `Conv2D` (with `relu` activation) and `MaxPooling2D` layers.\n",
    "\n",
    "However, since we are dealing with bigger images and a more complex problem, we will make our network accordingly larger: it will have one \n",
    "more `Conv2D` + `MaxPooling2D` stage. This serves both to augment the capacity of the network, and to further reduce the size of the \n",
    "feature maps, so that they aren't overly large when we reach the `Flatten` layer. Here, since we start from inputs of size 150x150 (a \n",
    "somewhat arbitrary choice), we end up with feature maps of size 7x7 right before the `Flatten` layer.\n",
    "\n",
    "Note that the depth of the feature maps is progressively increasing in the network (from 32 to 128), while the size of the feature maps is \n",
    "decreasing (from 148x148 to 7x7). This is a pattern that you will see in almost all convnets.\n",
    "\n",
    "Since we are attacking a binary classification problem, we are ending the network with a single unit (a `Dense` layer of size 1) and a \n",
    "`sigmoid` activation. This unit will encode the probability that the network is looking at one class or the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at how the dimensions of the feature maps change with every successive layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 148, 148, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 74, 74, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 72, 72, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 34, 34, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 17, 17, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 15, 15, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               3211776   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 3,453,121\n",
      "Trainable params: 3,453,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our compilation step, we'll go with the `RMSprop` optimizer as usual. Since we ended our network with a single sigmoid unit, we will \n",
    "use binary crossentropy as our loss (as a reminder, check out the table in Chapter 4, section 5 for a cheatsheet on what loss function to \n",
    "use in various situations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=1e-4),\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "\n",
    "As you already know by now, data should be formatted into appropriately pre-processed floating point tensors before being fed into our \n",
    "network. Currently, our data sits on a drive as JPEG files, so the steps for getting it into our network are roughly:\n",
    "\n",
    "* Read the picture files.\n",
    "* Decode the JPEG content to RBG grids of pixels.\n",
    "* Convert these into floating point tensors.\n",
    "* Rescale the pixel values (between 0 and 255) to the [0, 1] interval (as you know, neural networks prefer to deal with small input values).\n",
    "\n",
    "It may seem a bit daunting, but thankfully Keras has utilities to take care of these steps automatically. Keras has a module with image \n",
    "processing helper tools, located at `keras.preprocessing.image`. In particular, it contains the class `ImageDataGenerator` which allows to \n",
    "quickly set up Python generators that can automatically turn image files on disk into batches of pre-processed tensors. This is what we \n",
    "will use here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 175 images belonging to 5 classes.\n",
      "Found 90 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# All images will be rescaled by 1./255\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        # This is the target directory\n",
    "        train_dir,\n",
    "        # All images will be resized to 150x150\n",
    "        target_size=(150, 150),\n",
    "        batch_size=20,\n",
    "        # Since we use binary_crossentropy loss, we need binary labels\n",
    "        class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        validation_dir,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=20,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "100/100 [==============================] - 102s 1s/step - loss: -22298.8718 - acc: 0.3964 - val_loss: -65277.5625 - val_acc: 0.2000\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 96s 956ms/step - loss: -814072.9886 - acc: 0.4000 - val_loss: -1522752.6250 - val_acc: 0.2000\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 91s 911ms/step - loss: -6906655.0841 - acc: 0.4010 - val_loss: -9186324.0000 - val_acc: 0.2000\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 91s 913ms/step - loss: -31055204.5198 - acc: 0.3995 - val_loss: -44144576.0000 - val_acc: 0.2000\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 96s 959ms/step - loss: -103472633.5916 - acc: 0.3979 - val_loss: -157577088.0000 - val_acc: 0.2000\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 101s 1s/step - loss: -263147594.6713 - acc: 0.4021 - val_loss: -639915072.0000 - val_acc: 0.2000\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 94s 941ms/step - loss: -607339783.2269 - acc: 0.3964 - val_loss: -647052736.0000 - val_acc: 0.2000\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 94s 936ms/step - loss: -1230554199.6801 - acc: 0.4015 - val_loss: -974554944.0000 - val_acc: 0.2000\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 93s 926ms/step - loss: -2350152662.1163 - acc: 0.4005 - val_loss: -1505863296.0000 - val_acc: 0.2000\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 89s 891ms/step - loss: -4212042361.5787 - acc: 0.3974 - val_loss: 53003112.0000 - val_acc: 0.2000\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 102s 1s/step - loss: -6923603313.1179 - acc: 0.4041 - val_loss: -1135373056.0000 - val_acc: 0.2000\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 122s 1s/step - loss: -11299292966.7123 - acc: 0.3974 - val_loss: -12459674624.0000 - val_acc: 0.2000\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 92s 916ms/step - loss: -17316198688.4814 - acc: 0.4036 - val_loss: -34958925824.0000 - val_acc: 0.2000\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 106s 1s/step - loss: -26499375640.0467 - acc: 0.3938 - val_loss: -21975531520.0000 - val_acc: 0.2000\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 114s 1s/step - loss: -38223446113.2681 - acc: 0.3985 - val_loss: -56087289856.0000 - val_acc: 0.2000\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 107s 1s/step - loss: -55246737112.4124 - acc: 0.4036 - val_loss: -53290967040.0000 - val_acc: 0.2000\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 109s 1s/step - loss: -76548758918.9346 - acc: 0.4036 - val_loss: -66453807104.0000 - val_acc: 0.2000\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 109s 1s/step - loss: -107098496197.2326 - acc: 0.3979 - val_loss: -99016589312.0000 - val_acc: 0.2000\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 97s 971ms/step - loss: -144483194050.7187 - acc: 0.3979 - val_loss: -108152709120.0000 - val_acc: 0.2000\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 103s 1s/step - loss: -191457029173.2016 - acc: 0.4005 - val_loss: -85928230912.0000 - val_acc: 0.2000\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 105s 1s/step - loss: -253614190212.0359 - acc: 0.4021 - val_loss: -203965235200.0000 - val_acc: 0.2000\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 95s 950ms/step - loss: -324469826706.2260 - acc: 0.4010 - val_loss: -372972945408.0000 - val_acc: 0.2000\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 96s 959ms/step - loss: -421847633658.1638 - acc: 0.3990 - val_loss: -530686509056.0000 - val_acc: 0.2000\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 92s 925ms/step - loss: -535020305419.3929 - acc: 0.4005 - val_loss: 68210237440.0000 - val_acc: 0.2000\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 99s 985ms/step - loss: -673491373862.6323 - acc: 0.3985 - val_loss: -580753424384.0000 - val_acc: 0.2000\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 94s 940ms/step - loss: -840663030333.6527 - acc: 0.4010 - val_loss: -1306363559936.0000 - val_acc: 0.2000\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 93s 928ms/step - loss: -1039811141712.2749 - acc: 0.3995 - val_loss: -1340735225856.0000 - val_acc: 0.2000\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 93s 925ms/step - loss: -1276183581825.3521 - acc: 0.4015 - val_loss: -403969245184.0000 - val_acc: 0.2000\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 109s 1s/step - loss: -1567281854129.6084 - acc: 0.3995 - val_loss: -2157248249856.0000 - val_acc: 0.2000\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 99s 994ms/step - loss: -1893774789588.6235 - acc: 0.3995 - val_loss: -1634270052352.0000 - val_acc: 0.2000\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(\n",
    "      train_generator,\n",
    "      steps_per_epoch=100,\n",
    "      epochs=30,\n",
    "      validation_data=validation_generator,\n",
    "      validation_steps=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "It is good practice to always save your models after training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('coil-20-unproc_small.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the loss and accuracy of the model over the training and validation data during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'ro', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'ro', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
